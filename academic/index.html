<head>
    <meta charset="utf-8">
    <title>Tao He's Homepage</title>
    <!-- 可以在这里添加CSS或其他资源引用 -->
</head>

		<div class="blurb">
			<h3>Dr. Wei Shicai(韦仕才)</h3>
			<p><img src="1.jpg" alt="photo" style="width:160px;height:200px;float:left;margin-right:35px;">
				<ul class="contacts">
					<li>Laboratory of Intelligent Collaborative Computing</li>
					<li>University of Electronic Science and Technology of China (UESTC)</li>
					 
					<li> <b>Email</b>: shicaiwei123@gmail.com </li>
					<li><a href="https://github.com/shicaiwei123"><div style="color:blue;"> Github</div></a></li>
					 
				</ul>
			</p>
			</div> 

<div class="about">
	
	<p>I am a Deputy Reseacher(equivalent to Associate Professor) at <a = href="https://icct.uestc.edu.cn/index.htm"><font color="#0000ff">Laboratory of Intelligent Collaborative Computing</font></a>, UESTC. Before working at UESTC, I earned my Ph.D. at UESTC 2025 under the supervision of 
		<a = href="https://users.monash.edu.au/~yli/"><font color="#0000ff">Prof. Chunbo Luo</font></a>  
		and co-supervision with <a href="https://lianligao.github.io/"><font color="#0000ff">Assoc. Prof. Yang Luo</font>.</a> 
	 My research interests focus on multimodal learning and efficient learning, such as emotion recognition, knowledge distillation, etc.  

<h4> Recent News</h4>
<ul>
	<li>One paper was accepted by the IEEE Transactions on Geoscience and Remote Sensing ( <strong>TGRS 2024</strong>).</li>	
	<li>Two papers accepted to European Conference on Computer Vision ( <strong>ECCV 2024</strong>).</li>
	<li>One paper was accepted by the  IEEE / CVF Computer Vision and Pattern Recognition Conference ( <strong>CVPR 2024</strong>)</li>
	<li>One paper was accepted by the IEEE Transactions on Geoscience and Remote Sensing ( <strong>TGRS 2024</strong>).</li>	
	<li>One paper accepted by the  IEEE Transactions on Multimedia ( <strong>TMM 2023</strong>).</li>
	<li>One paper was accepted by the IEEE Transactions on Geoscience and Remote Sensing ( <strong>TGRS 2023</strong>).</li>
	<li>One paper was accepted by the IEEE / CVF Computer Vision and Pattern Recognition Conference ( <strong>CVPR 2023</strong>) </li>
<!-- 	<li>One paper acccepted to <strong>IJCAI 2021</strong>  on scene graph generation.</li> -->
<!-- 	<li>One paper accepted to <strong>AAAI 2020</strong>  on network quantisation.</li> -->
<!-- 	<li>One paper acccepted to <strong>IJCV 2020</strong>  on image compression.</li> -->
	<!--<li>One paper accepted to <strong>IJCAI 2019</strong>  on domain adaptive hash for images.</li>--!>
	</ul>

<h4>Selected Publications</h4>
<ul>
	<li><strong>S Wei</strong>, C Luo, Y Luo. MMANet: Margin-aware distillation and modality-aware regularization for incomplete multimodal learning. CVPR 2023. <a href="https://github.com/shicaiwei123/MMANet-CVPR2023" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wei_MMANet_Margin-Aware_Distillation_and_Modality-Aware_Regularization_for_Incomplete_Multimodal_Learning_CVPR_2023_paper.html" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li><strong>S Wei</strong>, C Luo, Y Luo. Scale decoupled distillation. CVPR 2024. <a href="https://github.com/shicaiwei123/SDD-CVPR2024" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://arxiv.org/abs/2403.13512" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li><strong>S Wei</strong>, Y Luo, Y Wang, C Luo.  Robust multimodal learning via representation decoupling. ECCV 2024. <a href="https://github.com/shicaiwei123/ECCV2024-DMRNet" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://arxiv.org/abs/2407.04458" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	<li><strong>S Wei</strong>, Y Luo, C Luo. Gradient Decoupled Learning with Unimodal Regularization for Multimodal Remote Sensing Classification. TGRS 2024. <a href="https://ieeexplore.ieee.org/document/10714439/ " rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://github.com/shicaiwei123/TGRS-GDNet" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>S Wei</strong>, Y Luo, X Ma, P Ren, C Luo.  MSH-Net: Modality-shared hallucination with joint adaptation distillation for remote sensing image classification using missing modalities. TGRS 2023.<a href="https://github.com/shicaiwei123/TGRS-MSHNet" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://ieeexplore.ieee.org/document/10097714" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	
	<li><strong>S Wei</strong>, C Luo, Y Luo, J Xu. Privileged modality learning via multimodal hallucination. TMM 2023. <a href="https://github.com/shicaiwei123/TMM-MMH" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://ieeexplore.ieee.org/document/10146467" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>

	<li><strong>S Wei</strong>, Y Luo, C Luo .Diversity-Guided Distillation With Modality-Center Regularization for Robust Multimodal Remote Sensing Image Classification. TGRS 2024.<a href="https://github.com/shicaiwei123/TGRS-DGDNet" rel="nofollow"><font color="#0000ff">[code]</font></a> <a href="https://ieeexplore.ieee.org/document/10330617" rel="nofollow"><font color="#0000ff">[paper]</font></a> <font color="#ff0000"><b>&nbsp;</b></font> </li>
	
</ul>

	
<h4> Acadamic Service </h4>
<ul>
	<li>As a reviewer for confereces: CVPR, ECCV, ICCV, CVPR, NIPS, ACMMM</li>
	<li>As a reviewer for journals: TPAMI, TMM, TGRS </li>
	</ul>
</div>
